import json
import logging
import uuid
from typing import Any, Dict, Generator

import requests
import streamlit as st

from ..utils import get_img_base64

AGENT_PAGE_INTRODUCTION = "ä½ å¥½ï¼Œæˆ‘æ˜¯ **Nova Agent** æ™ºèƒ½åŠ©æ‰‹ï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ"
LLM_OPTIONS = ["basic", "reasoning", "basic_no_thinking"]
BACKEND_URL = "http://0.0.0.0:2021/task/stream_deepresearcher"
AVATAR_PATH = "chat.png"

# æµå¼ä¼˜åŒ–å‚æ•°
MAX_TOTAL_CHARS = 150000  # æ€»å­—ç¬¦ä¿æŠ¤é™åˆ¶

logger = logging.getLogger(__name__)


def get_task_response(
    llm_dtype: str, messages: list, max_react_tool_calls: int
) -> Generator[Dict[str, Any], None, None]:
    """ç”Ÿæˆå¸¦ç±»å‹æ ‡è®°çš„æµå¼å“åº”ï¼ŒåŒºåˆ†æ€è€ƒå’Œå›ç­”å†…å®¹"""
    trace_id = str(uuid.uuid4())
    request_data = {
        "trace_id": trace_id,
        "context": {
            "clarify_model": llm_dtype,
            "research_brief_model": llm_dtype,
            "supervisor_model": llm_dtype,
            "researcher_model": llm_dtype,
            "summarize_model": llm_dtype,
            "compress_research_model": llm_dtype,
            "report_model": llm_dtype,
            "number_of_initial_queries": 1,
            "max_research_loops": 1,
            "max_concurrent_research_units": 2,
            "max_react_tool_calls": max_react_tool_calls,
        },
        "state": {"messages": messages},
    }

    current_answer_message_id = None
    current_reasoning_message_id = None

    try:
        with requests.post(
            BACKEND_URL,
            json=request_data,
            stream=True,
            headers={"Accept": "text/event-stream"},
            timeout=3600,
        ) as response:
            response.raise_for_status()

            for line in response.iter_lines(decode_unicode=True):
                if not line:
                    continue

                try:
                    line_data = json.loads(line)

                    if line_data.get("code", 1) != 0:
                        error_msg = f"âŒ åç«¯é”™è¯¯: {line_data.get('message', 'æœªçŸ¥é”™è¯¯')}(code={line_data.get('code')})"
                        logger.error(f"{error_msg} (trace_id: {trace_id})")
                        yield {"type": "error", "content": error_msg}
                        return

                    _event = line_data["messages"]["event"]
                    _data = line_data["messages"]["data"]
                    _node_name = _data["node_name"]
                    _step = _data["step"] or 0
                    # _run_id = _data["run_id"]
                    # _parent_ids = _data["parent_ids"]
                    # _checkpoint_ns = _data["checkpoint_ns"]
                    # _trace_id = _data["trace_id"]

                    # ç³»ç»Ÿäº‹ä»¶
                    if _event in [
                        "on_chain_start",
                        "on_chain_end",
                        "on_tool_start",
                        "on_tool_end",
                    ]:
                        if _event == "on_chain_start":
                            _graph_name = _node_name.split("|")[0]
                            if "RunnableSequence" in _node_name:
                                content = None
                            elif "LangGraph" in _node_name:
                                if _graph_name:
                                    content = f"â³ ã€å›¾{_graph_name}ã€‘å¼€å§‹ä»»åŠ¡\n\n"
                                else:
                                    content = "â³ ã€å›¾taskã€‘å¼€å§‹ä»»åŠ¡\n\n"
                            else:
                                content = f"ğŸ“Œ ã€å›¾{_graph_name}ã€‘ğŸš€ã€ç¬¬{_step}æ­¥æ‰§è¡Œ: {_node_name}ã€‘\n\n"

                        elif _event == "on_chain_end":
                            _graph_name = _node_name.split("|")[0]
                            if "RunnableSequence" in _node_name:
                                content = None
                            elif "LangGraph" in _node_name:
                                if _graph_name:
                                    content = (
                                        f"âœ… ã€å›¾{_graph_name}ã€‘ ğŸŸ¢ ã€ä»»åŠ¡ç»“æŸã€‘\n\n"
                                    )
                                else:
                                    content = "âœ… ã€å›¾taskã€‘ ğŸŸ¢ ã€ä»»åŠ¡ç»“æŸã€‘\n\n"
                            else:
                                content = f"â³ ã€å›¾{_graph_name}ã€‘ğŸŸ¢ã€ç¬¬{_step}æ­¥å®Œæˆã€‘: {_node_name}\n\n"

                        elif _event == "on_tool_start":
                            _input = str(_data["input"])
                            content = (
                                f"ğŸ› ï¸ ã€è°ƒç”¨å·¥å…·: {_node_name}ã€‘\n\nå…¥å‚: {_input[:200]}...\n\n"
                                if len(_input) > 200
                                else f"ğŸ› ï¸ ã€è°ƒç”¨å·¥å…·: {_node_name}ã€‘\n\nå…¥å‚: {_input}\n\n"
                            )

                        elif _event == "on_tool_end":
                            _output = str(_data["output"])
                            content = (
                                f"ğŸ› ï¸ ã€å·¥å…·: {_node_name}æ‰§è¡Œç»“æŸã€‘\n\nå‡ºå‚: {_output[:200]}...\n\n"
                                if len(_output) > 200
                                else f"ğŸ› ï¸ ã€å·¥å…·: {_node_name}æ‰§è¡Œç»“æŸã€‘\n\nå‡ºå‚: {_output}\n\n"
                            )

                        if content:
                            yield {"type": "system", "content": content}

                    elif _event in [
                        "on_chat_model_start",
                        "on_chat_model_end",
                        "on_chat_model_stream",
                    ]:
                        if _event == "on_chat_model_start":
                            content = f"ğŸ¤” ã€{_node_name}: æ­£åœ¨æ€è€ƒ...ã€‘\n\n"
                            if content:
                                yield {"type": "chat_start", "content": content}

                        elif _event == "on_chat_model_end":
                            # title = f"âœ¨ ã€{_node_name}: æ€è€ƒå®Œæˆã€‘\n\n"
                            reasoning_content = (
                                _data["output"].get("reasoning_content", "").strip()
                            )
                            content = _data["output"].get("content", "").strip()
                            tool_calls = _data["output"].get("tool_calls", [])

                            key_info = {
                                "content": content,
                                "reasoning_content": reasoning_content,
                                "tool_calls": tool_calls,
                            }

                            yield {"type": "chat_end", "content": key_info}

                        # æ¨¡å‹æµå¼äº‹ä»¶ - åŒºåˆ†æ€è€ƒå’Œå›ç­”å†…å®¹
                        elif _event == "on_chat_model_stream":
                            _output = _data["output"]
                            _message_id = _output["message_id"]
                            _reasoning = _output.get("reasoning_content", "")
                            _answer = _output.get("content", "")
                            # _tool_calls = _output.get("tool_calls", [])

                            # æ€è€ƒå†…å®¹
                            if _reasoning:
                                if _message_id != current_reasoning_message_id:
                                    current_reasoning_message_id = _message_id
                                    yield {
                                        "type": "thought",
                                        "content": "ğŸ“ æ€è€ƒè¿‡ç¨‹ï¼š\n\n",
                                    }

                                yield {"type": "thought", "content": f"{_reasoning}"}

                            # å›ç­”å†…å®¹
                            if _answer:
                                if _message_id != current_answer_message_id:
                                    current_answer_message_id = _message_id
                                    yield {
                                        "type": "answer",
                                        "content": "ğŸ“Œ å›ç­”å†…å®¹ï¼š\n\n",
                                    }

                                yield {"type": "answer", "content": f"{_answer}"}

                except json.JSONDecodeError:
                    error_msg = (
                        f"âŒ å“åº”æ ¼å¼é”™è¯¯: æ— æ³•è§£æå†…å®¹ï¼ˆå‰200å­—ç¬¦ï¼‰: {line[:200]}..."
                    )
                    logger.error(f"{error_msg} (trace_id: {trace_id})")
                    yield {"type": "error", "content": error_msg}
                    return
                except KeyError as e:
                    error_msg = f"âŒ å“åº”ç»“æ„é”™è¯¯: ç¼ºå°‘å¿…è¦å­—æ®µã€Œ{str(e)}ã€"
                    logger.error(f"{error_msg} (trace_id: {trace_id})")
                    yield {"type": "error", "content": error_msg}
                    return

    except requests.exceptions.RequestException as e:
        error_msg = f"âŒ è¯·æ±‚å¤±è´¥: {str(e)}ï¼ˆæµå¼è¿æ¥ä¸­æ–­ï¼‰"
        logger.error(f"{error_msg} (trace_id: {trace_id})")
        yield {"type": "error", "content": error_msg}
        return
    except Exception as e:
        error_msg = f"âŒ æµå¼å¤„ç†å¼‚å¸¸: {str(e)}"
        logger.error(f"{error_msg} (trace_id: {trace_id})")
        yield {"type": "error", "content": error_msg}
        return


def clear_task_history():
    st.session_state.chat_history = [
        {"role": "assistant", "content": AGENT_PAGE_INTRODUCTION}
    ]


def display_task_history():
    """æ˜¾ç¤ºå¯¹è¯å†å²è®°å½•"""
    # ç¡®ä¿ä¼šè¯çŠ¶æ€ä¸­æœ‰èŠå¤©å†å²
    if "chat_history" not in st.session_state:
        clear_task_history()

    for message in st.session_state["chat_history"]:
        # å¤„ç†å¤´åƒè·å–å¯èƒ½å‡ºç°çš„é”™è¯¯
        try:
            avatar = (
                get_img_base64(AVATAR_PATH) if message["role"] == "assistant" else None
            )
        except Exception as e:
            logger.warning(f"è·å–å¤´åƒå¤±è´¥: {e}")
            avatar = None

        with st.chat_message(message["role"], avatar=avatar):
            st.write(message["content"])


def llm_deepresearcher_page():
    """ä¸»é¡µé¢å‡½æ•°ï¼Œå®ç°æ€è€ƒè¿‡ç¨‹æµå¼å±•ç¤ºåè‡ªåŠ¨æŠ˜å """
    st.set_page_config(
        page_title="Nova æ™ºèƒ½åŠ©æ‰‹",
        page_icon=get_img_base64("nova_chat.png"),
        layout="wide",
    )

    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
    if "chat_history" not in st.session_state:
        clear_task_history()

    # ä¾§è¾¹æ é…ç½®
    with st.sidebar:
        global MAX_TOTAL_CHARS
        st.title("ğŸš€ é…ç½®")
        llm_type = st.selectbox("é€‰æ‹©æ¨¡å‹", LLM_OPTIONS, index=0)
        max_react_tool_calls = st.slider("å·¥å…·æœ€å¤§è°ƒç”¨æ¬¡æ•°", 1, 5, 2, 1)
        MAX_TOTAL_CHARS = st.slider("æœ€å¤§æ€»å­—ç¬¦é™åˆ¶ï¼ˆåƒï¼‰", 50, 300, 150, 10) * 1000

    # æ˜¾ç¤ºå†å²æ¶ˆæ¯
    display_task_history()

    # åº•éƒ¨è¾“å…¥æ¡†
    with st._bottom:
        cols = st.columns([10, 1])
        user_input = cols[0].chat_input("è¯·è¾“å…¥é—®é¢˜...")
        if cols[1].button(":wastebasket:", help="æ¸…ç©ºå¯¹è¯å†å²"):
            clear_task_history()
            st.rerun()

    # å¤„ç†ç”¨æˆ·è¾“å…¥
    if user_input:
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        with st.chat_message("user"):
            st.write(user_input)
        st.session_state.chat_history.append({"role": "user", "content": user_input})

        # æµå¼è·å–å¹¶å±•ç¤ºåŠ©æ‰‹å“åº”
        with st.chat_message("assistant", avatar=get_img_base64(AVATAR_PATH)):
            # # åˆ›å»ºå®¹å™¨ç”¨äºåŠ¨æ€å±•ç¤ºå’ŒæŠ˜å 
            sys_container = st.container()
            _temp = ""  # ä¸´æ—¶æµå¼å†…å®¹
            _placeholder = None  # ç”¨äºæµå¼æ›´æ–°çš„å ä½ç¬¦

            full_response = ""  # æ§åˆ¶è¾“å‡ºé•¿åº¦çš„å˜é‡
            final_answer = []
            stream_generator = get_task_response(
                llm_type, st.session_state.chat_history[1:], max_react_tool_calls
            )
            # å¤„ç†æµå¼å“åº”
            for item in stream_generator:
                content = item["content"]
                full_response += str(content)  # ç´¯åŠ å®Œæ•´å“åº”
                if len(full_response) >= MAX_TOTAL_CHARS:
                    full_response += "\n\nâš ï¸ å·²è¾¾åˆ°æœ€å¤§å­—ç¬¦é™åˆ¶ï¼Œåç»­å†…å®¹å·²æˆªæ–­ã€‚"
                    continue  # ç»ˆæ­¢æµå¼å¤„ç†
                # ğŸ”¹ å¤„ç† System æ¶ˆæ¯ï¼ˆå¦‚ä»»åŠ¡çŠ¶æ€ã€å·¥å…·è°ƒç”¨ï¼‰
                if item["type"] in ["system", "error"]:
                    with sys_container:
                        sys_container.markdown(content, unsafe_allow_html=False)
                    if item["type"] == "error":
                        with sys_container:
                            sys_container.markdown(
                                f"<span style='color:red'>{content}</span>",
                                unsafe_allow_html=True,
                            )
                elif item["type"] == "chat_start":
                    with sys_container:
                        sys_container.markdown(content, unsafe_allow_html=False)

                    # åˆå§‹åŒ–å ä½ç¬¦ï¼Œç”¨äºæµå¼æ›´æ–°
                    _placeholder = st.empty()

                elif item["type"] == "chat_end":
                    if _placeholder:
                        _placeholder.empty()  # æ¸…ç©ºå ä½ç¬¦
                        _placeholder = None  # é‡ç½®å ä½ç¬¦

                    if isinstance(content, dict):
                        _reasoning_content = content["reasoning_content"]
                        _content = content["content"]
                        _tool_calls = content["tool_calls"]

                        if _reasoning_content:
                            with sys_container:
                                #  ç”¨æŠ˜å é¢æ¿æ›¿æ¢ï¼Œé»˜è®¤ä¸å±•å¼€
                                with st.expander("æŸ¥çœ‹ğŸ“æ€è€ƒè¿‡ç¨‹", expanded=False):
                                    st.markdown(
                                        _reasoning_content, unsafe_allow_html=False
                                    )  # åŒ…å«å®Œæˆæ ‡è®°
                        if _tool_calls:
                            with sys_container:
                                #  ç”¨æŠ˜å é¢æ¿æ›¿æ¢ï¼Œé»˜è®¤ä¸å±•å¼€
                                with st.expander("æŸ¥çœ‹ğŸ“å·¥å…·å…¥å‚", expanded=False):
                                    st.markdown(
                                        _tool_calls, unsafe_allow_html=False
                                    )  # åŒ…å«å®Œæˆæ ‡è®°

                        if _content:
                            final_answer.append(_content)
                            with sys_container:
                                _content = f"ğŸ“˜ ã€Answerã€‘\n\n{_content}\n\n"
                                sys_container.markdown(
                                    _content, unsafe_allow_html=False
                                )

                # ğŸ”¹ å¤„ç†å›ç­”å†…å®¹ï¼ˆæµå¼å®æ—¶æ˜¾ç¤ºï¼‰
                elif item["type"] == "answer":
                    # ç´¯åŠ å¹¶ä½¿ç”¨å ä½ç¬¦æ›´æ–°ï¼ˆé¿å…é—ªçƒï¼Œæ¯1ä¸ªå­—ç¬¦æ›´æ–°ä¸€æ¬¡ä»¥å®ç°æ›´å¹³æ»‘çš„æµå¼æ•ˆæœï¼‰
                    _temp += content
                    if _placeholder:
                        _placeholder.markdown(_temp, unsafe_allow_html=False)

                # ğŸ”¹ å¤„ç†æ€è€ƒå†…å®¹ï¼ˆæµå¼å®æ—¶æ˜¾ç¤ºï¼‰
                elif item["type"] == "thought":
                    # ç´¯åŠ å¹¶ä½¿ç”¨å ä½ç¬¦æ›´æ–°ï¼ˆé¿å…é—ªçƒï¼Œæ¯1ä¸ªå­—ç¬¦æ›´æ–°ä¸€æ¬¡ä»¥å®ç°æ›´å¹³æ»‘çš„æµå¼æ•ˆæœï¼‰
                    _temp += content
                    if _placeholder:
                        _placeholder.markdown(_temp, unsafe_allow_html=False)

        # æ·»åŠ å®Œæ•´å“åº”åˆ°å†å²
        st.session_state.chat_history.append(
            {"role": "assistant", "content": final_answer[-1]}
        )
        # st.rerun()

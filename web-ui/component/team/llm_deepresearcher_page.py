import json
import logging
import uuid
from collections import deque
from typing import Any, Dict, Generator

import requests
import streamlit as st
from utils import get_img_base64

AGENT_PAGE_INTRODUCTION = "ä½ å¥½ï¼Œæˆ‘æ˜¯ **Nova Agent** æ™ºèƒ½åŠ©æ‰‹ï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ"
LLM_OPTIONS = ["basic", "reasoning", "basic_no_thinking"]
BACKEND_URL = "http://0.0.0.0:2021/task/stream_deepresearcher"
AVATAR_PATH = "chat.png"

# æµå¼ä¼˜åŒ–å‚æ•°
MAX_TOTAL_CHARS = 150000  # æ€»å­—ç¬¦ä¿æŠ¤é™åˆ¶

logger = logging.getLogger(__name__)


class Stack:
    def __init__(self):
        self.items = deque()  # ä½¿ç”¨åŒç«¯é˜Ÿåˆ—å®ç°æ ˆ

    def push(self, item):
        """å…¥æ ˆæ“ä½œ"""
        self.items.append(item)

    def pop(self):
        """å‡ºæ ˆæ“ä½œ"""
        if not self.is_empty():
            return self.items.pop()
        else:
            raise IndexError("æ ˆä¸ºç©ºï¼Œæ— æ³•å‡ºæ ˆ")

    def peek(self):
        """æŸ¥çœ‹æ ˆé¡¶å…ƒç´ """
        if not self.is_empty():
            return self.items[-1]
        else:
            raise IndexError("æ ˆä¸ºç©ºï¼Œæ— æ³•æŸ¥çœ‹æ ˆé¡¶å…ƒç´ ")

    def is_empty(self):
        """åˆ¤æ–­æ ˆæ˜¯å¦ä¸ºç©º"""
        return len(self.items) == 0

    def size(self):
        """è¿”å›æ ˆçš„å¤§å°"""
        return len(self.items)


def get_task_response(
    llm_dtype: str, messages: list, max_react_tool_calls: int
) -> Generator[Dict[str, Any], None, None]:
    """ç”Ÿæˆå¸¦ç±»å‹æ ‡è®°çš„æµå¼å“åº”ï¼ŒåŒºåˆ†æ€è€ƒå’Œå›ç­”å†…å®¹"""
    trace_id = str(uuid.uuid4())
    request_data = {
        "trace_id": trace_id,
        "context": {
            "clarify_model": llm_dtype,
            "research_brief_model": llm_dtype,
            "supervisor_model": llm_dtype,
            "researcher_model": llm_dtype,
            "summarize_model": llm_dtype,
            "compress_research_model": llm_dtype,
            "report_model": llm_dtype,
            "number_of_initial_queries": 1,
            "max_research_loops": 1,
            "max_concurrent_research_units": 2,
            "max_react_tool_calls": max_react_tool_calls,
        },
        "state": {"messages": messages},
    }

    current_answer_message_id = None
    current_reasoning_message_id = None
    reasoning_complete = False
    answer_complete = False

    try:
        _graph_stack = Stack()

        with requests.post(
            BACKEND_URL,
            json=request_data,
            stream=True,
            headers={"Accept": "text/event-stream"},
            timeout=600,
        ) as response:
            response.raise_for_status()

            for line in response.iter_lines(decode_unicode=True):
                if not line:
                    continue

                try:
                    line_data = json.loads(line)

                    if line_data.get("code", 1) != 0:
                        error_msg = f"âŒ åç«¯é”™è¯¯: {line_data.get('message', 'æœªçŸ¥é”™è¯¯')}(code={line_data.get('code')})"
                        logger.error(f"{error_msg} (trace_id: {trace_id})")
                        yield {"type": "error", "content": error_msg}
                        return

                    _event = line_data["messages"]["event"]
                    _data = line_data["messages"]["data"]
                    _node_name = _data["node_name"]
                    _step = _data["step"] or 0
                    # _run_id = _data["run_id"]
                    # _parent_ids = _data["parent_ids"]
                    # _checkpoint_ns = _data["checkpoint_ns"]
                    # _trace_id = _data["trace_id"]

                    # ç³»ç»Ÿäº‹ä»¶
                    if _event in [
                        "on_chain_start",
                        "on_chain_end",
                        "on_chat_model_start",
                        "on_chat_model_end",
                    ]:
                        if _event == "on_chain_start":
                            if "LangGraph" in _node_name:
                                try:
                                    _graph_name = _node_name.split("|")[0]
                                    _peek = _graph_stack.peek()
                                    _graph_stack.push(_graph_name)
                                except Exception:
                                    _graph_stack.push("task")

                                _peek = _graph_stack.peek()
                                content = f"â³ ã€å›¾{_peek}ã€‘å¼€å§‹ä»»åŠ¡\n\n"

                            elif "RunnableSequence" in _node_name:
                                content = None

                            else:
                                _peek = _graph_stack.peek()
                                content = f"ğŸ“Œ ã€å›¾{_peek}ã€‘ğŸš€ã€ç¬¬{_step}æ­¥æ‰§è¡Œ: {_node_name}ã€‘\n\n"

                        elif _event == "on_chain_end":
                            _peek = _graph_stack.peek()
                            if "LangGraph" in _node_name:
                                content = f"âœ… ã€å›¾{_peek}ã€‘ ğŸŸ¢ ã€ä»»åŠ¡ç»“æŸã€‘\n\n"
                                _graph_stack.pop()
                            else:
                                content = f"â³ ã€å›¾{_peek}ã€‘ğŸŸ¢ã€ç¬¬{_step}æ­¥å®Œæˆã€‘: {_node_name}\n\n"

                        elif _event == "on_chat_model_start":
                            content = f"ğŸ¤” ã€{_node_name}: æ­£åœ¨æ€è€ƒ...ã€‘\n\n"

                        elif _event == "on_chat_model_end":
                            content = f"âœ¨ ã€{_node_name}: æ€è€ƒå®Œæˆã€‘\n\n"
                            tmp = _data["output"].get("reasoning_content", "").strip()
                            if tmp:
                                content += f"â„¹ï¸ ã€Thinkã€‘\n\n{tmp}\n\n"

                            tmp = _data["output"].get("content", "").strip()
                            if tmp:
                                content += f"ğŸ“˜ ã€Answerã€‘\n\n{tmp}\n\n"

                            tmp = str(_data["output"].get("tool_calls", ""))
                            if tmp:
                                content += f"ğŸ› ï¸ ã€tool_callsã€‘\n\n{tmp}\n\n"

                        if content:
                            yield {"type": "system", "content": content}

                    # å·¥å…·äº‹ä»¶
                    elif _event in ["on_tool_start", "on_tool_end"]:
                        if _event == "on_tool_start":
                            _input = str(_data["input"])
                            content = (
                                f"ğŸ› ï¸ ã€è°ƒç”¨å·¥å…·: {_node_name}ã€‘\n\nå…¥å‚: {_input[:200]}...\n\n"
                                if len(_input) > 200
                                else f"ğŸ› ï¸ ã€è°ƒç”¨å·¥å…·: {_node_name}ã€‘\n\nå…¥å‚: {_input}\n\n"
                            )

                        else:
                            _output = str(_data["output"])
                            content = (
                                f"ğŸ› ï¸ ã€å·¥å…·: {_node_name}æ‰§è¡Œç»“æŸã€‘\n\nå‡ºå‚: {_output[:200]}...\n\n"
                                if len(_output) > 200
                                else f"ğŸ› ï¸ ã€å·¥å…·: {_node_name}æ‰§è¡Œç»“æŸã€‘\n\nå‡ºå‚: {_output}\n\n"
                            )

                        yield {"type": "system", "content": content}

                    # æ¨¡å‹æµå¼äº‹ä»¶ - åŒºåˆ†æ€è€ƒå’Œå›ç­”å†…å®¹
                    # elif _event == "on_chat_model_stream":
                    #     _output = _data["output"]
                    #     _message_id = _output["message_id"]
                    #     _reasoning = _output.get("reasoning_content", "")
                    #     _answer = _output.get("content", "")

                    #     # æ€è€ƒå†…å®¹
                    #     if _reasoning:
                    #         reasoning_complete = True
                    #         if _message_id != current_reasoning_message_id:
                    #             current_reasoning_message_id = _message_id
                    #             yield {
                    #                 "type": "thought_start",
                    #                 "content": "ğŸ“ æ€è€ƒè¿‡ç¨‹ï¼š\n\n",
                    #             }
                    #         else:
                    #             yield {"type": "thought", "content": f"{_reasoning}"}
                    #     elif reasoning_complete:
                    #         reasoning_complete = False
                    #         yield {"type": "thought_complete", "content": ""}

                    #     # å›ç­”å†…å®¹
                    #     if _answer:
                    #         answer_complete = True
                    #         if _message_id != current_answer_message_id:
                    #             current_answer_message_id = _message_id
                    #             yield {
                    #                 "type": "answer_start",
                    #                 "content": "ğŸ“Œ å›ç­”å†…å®¹ï¼š\n\n",
                    #             }
                    #         else:
                    #             yield {"type": "answer", "content": f"{_answer}"}
                    #     elif answer_complete:
                    #         answer_complete = False
                    #         yield {"type": "answer_complete", "content": ""}

                except json.JSONDecodeError:
                    error_msg = (
                        f"âŒ å“åº”æ ¼å¼é”™è¯¯: æ— æ³•è§£æå†…å®¹ï¼ˆå‰200å­—ç¬¦ï¼‰: {line[:200]}..."
                    )
                    logger.error(f"{error_msg} (trace_id: {trace_id})")
                    yield {"type": "error", "content": error_msg}
                    return
                except KeyError as e:
                    error_msg = f"âŒ å“åº”ç»“æ„é”™è¯¯: ç¼ºå°‘å¿…è¦å­—æ®µã€Œ{str(e)}ã€"
                    logger.error(f"{error_msg} (trace_id: {trace_id})")
                    yield {"type": "error", "content": error_msg}
                    return

    except requests.exceptions.RequestException as e:
        error_msg = f"âŒ è¯·æ±‚å¤±è´¥: {str(e)}ï¼ˆæµå¼è¿æ¥ä¸­æ–­ï¼‰"
        logger.error(f"{error_msg} (trace_id: {trace_id})")
        yield {"type": "error", "content": error_msg}
        return
    except Exception as e:
        error_msg = f"âŒ æµå¼å¤„ç†å¼‚å¸¸: {str(e)}"
        logger.error(f"{error_msg} (trace_id: {trace_id})")
        yield {"type": "error", "content": error_msg}
        return


def clear_agent_history():
    st.session_state.chat_history = [
        {"role": "assistant", "content": AGENT_PAGE_INTRODUCTION}
    ]


def display_agent_history():
    """æ˜¾ç¤ºå¯¹è¯å†å²ï¼ŒæŠ˜å å·²å®Œæˆçš„æ€è€ƒè¿‡ç¨‹"""
    if "chat_history" not in st.session_state:
        clear_agent_history()

    for msg in st.session_state["chat_history"]:
        try:
            avatar = get_img_base64(AVATAR_PATH) if msg["role"] == "assistant" else None
        except Exception as e:
            logger.warning(f"å¤´åƒåŠ è½½å¤±è´¥: {e}")
            avatar = None

        with st.chat_message(msg["role"], avatar=avatar):
            # æ£€æŸ¥æ˜¯å¦æ˜¯åŠ©æ‰‹æ¶ˆæ¯ä¸”åŒ…å«æ€è€ƒè¿‡ç¨‹
            if msg["role"] == "assistant" and "ğŸ“ æ€è€ƒè¿‡ç¨‹ï¼š" in msg["content"]:
                # åˆ†å‰²æ€è€ƒå’Œå›ç­”éƒ¨åˆ†
                thought_start = msg["content"].find("ğŸ“ æ€è€ƒè¿‡ç¨‹ï¼š")
                answer_start = msg["content"].find("ğŸ“Œ å›ç­”å†…å®¹ï¼š")

                if thought_start != -1 and answer_start != -1:
                    thought_content = msg["content"][thought_start:answer_start]
                    answer_content = msg["content"][answer_start:]

                    # æŠ˜å æ€è€ƒè¿‡ç¨‹ï¼Œé»˜è®¤ä¸å±•å¼€
                    with st.expander("æŸ¥çœ‹æ€è€ƒè¿‡ç¨‹", expanded=False):
                        st.markdown(thought_content, unsafe_allow_html=False)
                    st.markdown(answer_content, unsafe_allow_html=False)
                else:
                    st.markdown(msg["content"], unsafe_allow_html=False)
            else:
                st.markdown(msg["content"], unsafe_allow_html=False)


def llm_deepresearcher_page():
    """ä¸»é¡µé¢å‡½æ•°ï¼Œå®ç°æ€è€ƒè¿‡ç¨‹æµå¼å±•ç¤ºåè‡ªåŠ¨æŠ˜å """
    st.set_page_config(
        page_title="Nova æ™ºèƒ½åŠ©æ‰‹",
        page_icon=get_img_base64("nova_chat.png"),
        layout="wide",
    )

    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
    if "chat_history" not in st.session_state:
        clear_agent_history()

    # ä¾§è¾¹æ é…ç½®
    with st.sidebar:
        global MAX_TOTAL_CHARS
        st.title("ğŸš€ é…ç½®")
        llm_type = st.selectbox("é€‰æ‹©æ¨¡å‹", LLM_OPTIONS, index=0)
        max_react_tool_calls = st.slider("å·¥å…·æœ€å¤§è°ƒç”¨æ¬¡æ•°", 1, 5, 2, 1)
        MAX_TOTAL_CHARS = st.slider("æœ€å¤§æ€»å­—ç¬¦é™åˆ¶ï¼ˆåƒï¼‰", 50, 300, 150, 10) * 1000

    # æ˜¾ç¤ºå†å²æ¶ˆæ¯
    display_agent_history()

    # åº•éƒ¨è¾“å…¥æ¡†
    with st._bottom:
        cols = st.columns([10, 1])
        user_input = cols[0].chat_input("è¯·è¾“å…¥é—®é¢˜...")
        if cols[1].button(":wastebasket:", help="æ¸…ç©ºå¯¹è¯å†å²"):
            clear_agent_history()
            st.rerun()

    # å¤„ç†ç”¨æˆ·è¾“å…¥
    if user_input:
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        with st.chat_message("user"):
            st.write(user_input)
        st.session_state.chat_history.append({"role": "user", "content": user_input})

        # æµå¼è·å–å¹¶å±•ç¤ºåŠ©æ‰‹å“åº”
        with st.chat_message("assistant", avatar=get_img_base64(AVATAR_PATH)):
            # # åˆ›å»ºå®¹å™¨ç”¨äºåŠ¨æ€å±•ç¤ºå’ŒæŠ˜å 
            sys_container = st.container()
            thought_container = st.container()  # æ€è€ƒè¿‡ç¨‹å®¹å™¨
            answer_container = st.container()  # å›ç­”å†…å®¹å®¹å™¨
            temp_thought = ""  # ä¸´æ—¶å­˜å‚¨æ€è€ƒå†…å®¹
            temp_answer = ""  # ç´¯åŠ å­˜å‚¨å®Œæ•´å›ç­”å†…å®¹
            full_response = ""  # å®Œæ•´å“åº”å†…å®¹

            stream_generator = get_task_response(
                llm_type, st.session_state.chat_history[1:], max_react_tool_calls
            )

            # å¤„ç†æµå¼å“åº”
            for item in stream_generator:
                content = item["content"]
                full_response += content  # ç´¯åŠ å®Œæ•´å“åº”
                if len(full_response) >= MAX_TOTAL_CHARS:
                    full_response += "\n\nâš ï¸ å·²è¾¾åˆ°æœ€å¤§å­—ç¬¦é™åˆ¶ï¼Œåç»­å†…å®¹å·²æˆªæ–­ã€‚"
                    break  # ç»ˆæ­¢æµå¼å¤„ç†
                # ğŸ”¹ å¤„ç† System æ¶ˆæ¯ï¼ˆå¦‚ä»»åŠ¡çŠ¶æ€ã€å·¥å…·è°ƒç”¨ï¼‰
                if item["type"] in ["system", "error"]:
                    with sys_container:
                        sys_container.markdown(content, unsafe_allow_html=False)
                    if item["type"] == "error":
                        with answer_container:
                            sys_container.markdown(
                                f"<span style='color:red'>{content}</span>",
                                unsafe_allow_html=True,
                            )

                # # å¤„ç†å›ç­”å†…å®¹
                # elif item["type"] in ["answer_start", "answer"]:
                #     # ç´¯åŠ å›ç­”å†…å®¹
                #     temp_answer += content
                #     with answer_container:
                #         st.markdown(temp_answer, unsafe_allow_html=False)

                # # ğŸ”¹ å¤„ç†æ€è€ƒè¿‡ç¨‹ï¼ˆæµå¼å®æ—¶æ˜¾ç¤ºï¼‰
                # elif item["type"] in ["thought_start", "thought"]:
                #     # åœ¨ä¸´æ—¶å®¹å™¨ä¸­å®æ—¶æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹
                #     temp_thought += content
                #     if len(temp_thought) % 10 == 0 or item["type"] == "thought_start":
                #         with thought_container:
                #             st.markdown(temp_thought, unsafe_allow_html=True)

                # # æ€è€ƒå®Œæˆï¼Œæ›¿æ¢ä¸ºæŠ˜å å®¹å™¨
                # elif item["type"] == "thought_complete":
                #     with thought_container:
                #         st.empty()  # æ¸…ç©ºä¸´æ—¶å±•ç¤º
                #         # ç”¨æŠ˜å é¢æ¿æ›¿æ¢
                #         with st.expander("æŸ¥çœ‹æ€è€ƒè¿‡ç¨‹", expanded=False):
                #             st.markdown(temp_thought, unsafe_allow_html=False)

            # # ç¡®ä¿æœ€ç»ˆæ€è€ƒè¿‡ç¨‹è¢«æŠ˜å 
            # if temp_thought:
            #     with thought_container:
            #         st.empty()
            #         with st.expander("æŸ¥çœ‹æ€è€ƒè¿‡ç¨‹", expanded=False):
            #             st.markdown(temp_thought, unsafe_allow_html=False)

        # æ·»åŠ å®Œæ•´å“åº”åˆ°å†å²
        st.session_state.chat_history.append(
            {"role": "assistant", "content": full_response}
        )
        st.rerun()

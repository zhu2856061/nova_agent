import json
import logging
import uuid

import requests
import streamlit as st
from utils import get_img_base64

AGENT_PAGE_INTRODUCTION = "ä½ å¥½ï¼Œæˆ‘æ˜¯ **Nova Agent** æ™ºèƒ½åŠ©æ‰‹ï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ"

LLM_OPTIONS = ["basic", "reasoning", "basic_no_thinking"]

# åç«¯æ¥å£åœ°å€
BACKEND_URL = "http://0.0.0.0:2021/task/stream_deepresearcher"  # éœ€æ ¹æ®å®é™…ä¿®æ”¹
AVATAR_PATH = "chat.png"

logger = logging.getLogger(__name__)


def get_task_response(llm_dtype: str, messages: list, max_react_tool_calls: int):
    """
    å‘é€è¯·æ±‚åˆ°åç«¯å¹¶è·å–æµå¼å“åº”

    Args:
        llm_dtype: æ¨¡å‹ç±»å‹
        messages: å¯¹è¯å†å²æ¶ˆæ¯
        temperature: æ¨¡å‹æ¸©åº¦å‚æ•°

    Yields:
        æµå¼è¿”å›çš„å“åº”å†…å®¹ç‰‡æ®µ
    """
    trace_id = str(uuid.uuid4())
    request_data = {
        "trace_id": trace_id,
        "context": {
            "clarify_model": llm_dtype,
            "research_brief_model": llm_dtype,
            "supervisor_model": llm_dtype,
            "researcher_model": llm_dtype,
            "summarize_model": llm_dtype,
            "compress_research_model": llm_dtype,
            "report_model": llm_dtype,
            "number_of_initial_queries": 1,
            "max_research_loops": 1,
            "max_concurrent_research_units": 2,
            "max_react_tool_calls": max_react_tool_calls,
        },
        "state": {
            "messages": messages,
        },
    }
    try:
        # å‘é€POSTè¯·æ±‚å¹¶è®¾ç½®stream=Trueä»¥æ¥æ”¶æµå¼å“åº”
        with requests.post(
            BACKEND_URL,
            json=request_data,
            stream=True,
            headers={"Accept": "text/event-stream"},
            timeout=600,  # æ·»åŠ è¶…æ—¶è®¾ç½®
        ) as response:
            response.raise_for_status()  # æ£€æŸ¥HTTPé”™è¯¯çŠ¶æ€ç 
            _reasoning_content_message_id = ""
            _content_message_id = ""
            # é€è¡Œå¤„ç†æµå¼å“åº”
            for line in response.iter_lines(decode_unicode=True):
                if not line:
                    continue  # è·³è¿‡ç©ºè¡Œ
                try:
                    line = json.loads(line)

                    # æ£€æŸ¥åç«¯è¿”å›çš„çŠ¶æ€ç 
                    if line.get("code", 1) != 0:
                        error_msg = f"åç«¯é”™è¯¯: {line.get('message', 'æœªçŸ¥é”™è¯¯')}(code={line.get('code')})"
                        logger.error(f"{error_msg} (trace_id: {trace_id})")
                        yield error_msg
                        return

                    # æå–å†…å®¹ï¼ˆæ ¹æ®å®é™…åç«¯å“åº”ç»“æ„è°ƒæ•´ï¼‰
                    _event = line["messages"]["event"]
                    _data = line["messages"]["data"]
                    _node_name = _data["node_name"]
                    _step = _data["step"]
                    _run_id = _data["run_id"]
                    _trace_id = _data["trace_id"]

                    if _event == "on_chain_start":
                        if _node_name == "LangGraph":
                            yield "# å¼€å§‹ä»»åŠ¡\n\n"
                        else:
                            yield f"## æ‰§è¡Œç¬¬{_step}æ­¥ \n\n"

                    elif _event == "on_chain_end":
                        if _node_name == "LangGraph":
                            yield "# ä»»åŠ¡ç»“æŸ\n\n"
                        else:
                            yield f"## ç¬¬{_step}æ­¥å®Œæˆ\n\n"

                    elif _event == "on_chat_model_start":
                        yield f"### {_node_name}æ€è€ƒä¸­\n\n"

                    elif _event == "on_chat_model_end":
                        yield f"### {_node_name}æ€è€ƒå®Œæˆ\n\n"

                    elif _event == "on_tool_start":
                        _input = _data["input"]
                        yield (
                            f"### å¼€å§‹è°ƒç”¨å·¥å…·{_node_name}, å·¥å…·çš„å…¥å‚æ˜¯{_input} \n\n"
                        )

                    elif _event == "on_tool_end":
                        _output = _data["output"]
                        yield (
                            f"### å·¥å…·{_node_name}æ‰§è¡Œç»“æŸ, å·¥å…·çš„å‡ºå‚æ˜¯{_output}"
                            + "\n\n"
                        )

                    elif _event == "on_chat_model_stream":
                        _output = _data["output"]
                        _message_id = _output["message_id"]
                        _reasoning_content = _output.get("reasoning_content", "")
                        _content = _output.get("content", "")

                        if _reasoning_content:
                            if _message_id != _reasoning_content_message_id:
                                _reasoning_content_message_id = _message_id
                                yield f"#### Think\n\n {_reasoning_content}"
                            else:
                                yield _reasoning_content

                        if _content:
                            if _message_id != _content_message_id:
                                _content_message_id = _message_id
                                yield f"#### Answer\n\n {_reasoning_content}"
                            else:
                                yield _content

                except json.JSONDecodeError:
                    error_msg = f"å“åº”æ ¼å¼é”™è¯¯: æ— æ³•è§£æå†…å®¹ - {line}"
                    logger.error(f"{error_msg} (trace_id: {trace_id})")
                    yield error_msg
                    return
                except KeyError as e:
                    error_msg = f"å“åº”ç»“æ„é”™è¯¯: ç¼ºå°‘å­—æ®µ {str(e)}"
                    logger.error(f"{error_msg} (trace_id: {trace_id})")
                    yield error_msg
                    return

    except requests.exceptions.RequestException as e:
        error_msg = f"è¯·æ±‚å¤±è´¥: {str(e)}"
        logger.error(f"{error_msg} (trace_id: {trace_id})")
        yield error_msg
        return
    except Exception as e:
        error_msg = f"å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {str(e)}"
        logger.error(f"{error_msg} (trace_id: {trace_id})")
        yield error_msg
        return


def clear_agent_history():
    st.session_state.chat_history = [
        {"role": "assistant", "content": AGENT_PAGE_INTRODUCTION}
    ]


def display_agent_history():
    """æ˜¾ç¤ºå¯¹è¯å†å²è®°å½•"""
    # ç¡®ä¿ä¼šè¯çŠ¶æ€ä¸­æœ‰èŠå¤©å†å²
    if "chat_history" not in st.session_state:
        clear_agent_history()

    for message in st.session_state["chat_history"]:
        # å¤„ç†å¤´åƒè·å–å¯èƒ½å‡ºç°çš„é”™è¯¯
        try:
            avatar = (
                get_img_base64(AVATAR_PATH) if message["role"] == "assistant" else None
            )
        except Exception as e:
            logger.warning(f"è·å–å¤´åƒå¤±è´¥: {e}")
            avatar = None

        with st.chat_message(message["role"], avatar=avatar):
            st.write(message["content"])


def llm_task_page():
    """èŠå¤©é¡µé¢ä¸»å‡½æ•°"""
    st.set_page_config(
        page_title="Nova æ™ºèƒ½åŠ©æ‰‹",
        page_icon=get_img_base64("nova_chat.png"),
        layout="wide",
    )

    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
    if "chat_history" not in st.session_state:
        clear_agent_history()

    # ä¾§è¾¹æ é…ç½®
    with st.sidebar:
        st.title("ğŸ¤– æ¨¡å‹é…ç½®")
        llm_type = st.selectbox("é€‰æ‹©æ¨¡å‹", LLM_OPTIONS, index=0)
        max_react_tool_calls = st.slider(
            "æ‰§è¡Œå·¥å…·æ¬¡æ•°",
            min_value=1,
            max_value=5,
            value=2,
            step=1,
        )

    # æ˜¾ç¤ºå¯¹è¯å†å²
    display_agent_history()

    # åº•éƒ¨è¾“å…¥æ¡†
    with st._bottom:
        cols = st.columns([10, 1])
        user_input = cols[0].chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜...")
        if cols[1].button(label=":wastebasket:", help="æ¸…ç©ºå½“å‰å¯¹è¯"):
            clear_agent_history()
            st.rerun()

    # å¤„ç†ç”¨æˆ·è¾“å…¥
    if user_input:
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²å¹¶æ˜¾ç¤º
        with st.chat_message("user"):
            st.write(user_input)
        st.session_state.chat_history.append({"role": "user", "content": user_input})

        history_messages = st.session_state.chat_history[1:]

        # è·å–æ¨¡å‹å“åº”å¹¶æµå¼æ˜¾ç¤º
        with st.chat_message("assistant", avatar=get_img_base64(AVATAR_PATH)):
            response_generator = get_task_response(
                llm_type, history_messages, max_react_tool_calls
            )
            full_response = st.write_stream(response_generator)

        # æ·»åŠ åŠ©æ‰‹å“åº”åˆ°å†å²
        st.session_state.chat_history.append(
            {"role": "assistant", "content": full_response}
        )

        # æ»šåŠ¨åˆ°åº•éƒ¨
        st.rerun()

# -*- coding: utf-8 -*-
# @Time   : 2025/01/27 15:30
# @Author : Assistant
# @Moto   : å¢å¼ºç‰ˆWebå·¥å…·ï¼Œæ•´åˆç½‘ç»œæœç´¢å’Œå†…å®¹æŠ“å–åŠŸèƒ½

import logging
from typing import List

from langchain_core.tools import tool
from pydantic import Field

logger = logging.getLogger(__name__)


@tool(
    "enhanced_web_search_tool",
    description="""æ‰§è¡Œç½‘ç»œæœç´¢å¹¶è¿”å›ç›¸å…³ç»“æœåˆ—è¡¨ã€‚
    æ”¯æŒå¤šç§æœç´¢å¼•æ“ï¼Œè¿”å›æ ‡é¢˜ã€é“¾æ¥å’Œæ‘˜è¦ä¿¡æ¯ã€‚
    å½“éœ€è¦è·å–æœ€æ–°ä¿¡æ¯æˆ–æœç´¢ç‰¹å®šå†…å®¹æ—¶ä½¿ç”¨æ­¤å·¥å…·ã€‚""",
)
def enhanced_web_search_tool(
    query: str = Field(..., description="è¦æœç´¢çš„æŸ¥è¯¢å…³é”®è¯"),
    num_results: int = Field(default=5, description="è¿”å›çš„æœç´¢ç»“æœæ•°é‡ï¼Œé»˜è®¤5ä¸ª"),
    search_engine: str = Field(
        default="duckduckgo", description="ä½¿ç”¨çš„æœç´¢å¼•æ“ï¼šduckduckgo, google, baidu"
    ),
):
    """
    æ‰§è¡ŒWebæœç´¢

    Args:
        query: æœç´¢æŸ¥è¯¢
        num_results: ç»“æœæ•°é‡
        search_engine: æœç´¢å¼•æ“é€‰æ‹©

    Returns:
        str: æœç´¢ç»“æœ
    """
    logger.info(f"ğŸ” å¼€å§‹æœç´¢: {query} (å¼•æ“: {search_engine})")

    try:
        # ä½¿ç”¨DuckDuckGoæœç´¢ï¼ˆå…è´¹ä¸”æ— éœ€APIå¯†é’¥ï¼‰
        from duckduckgo_search import DDGS

        results = []
        with DDGS() as ddgs:
            search_results = ddgs.text(query, max_results=num_results)
            print("===>", search_results)
            for i, result in enumerate(search_results, 1):
                title = result.get("title", "N/A")
                href = result.get("href", "N/A")
                body = result.get("body", "N/A")

                results.append(f"{i}. **{title}**\n   é“¾æ¥: {href}\n   æ‘˜è¦: {body}\n")

        if results:
            result_text = f"æœç´¢æŸ¥è¯¢: {query}\n\næœç´¢ç»“æœ:\n\n" + "\n".join(results)
            logger.info(f"âœ… æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
            return result_text
        else:
            return f"æœªæ‰¾åˆ°ä¸æŸ¥è¯¢ '{query}' ç›¸å…³çš„ç»“æœ"

    except Exception as e:
        error_msg = f"æœç´¢å¤±è´¥: {str(e)}"
        logger.error(error_msg)
        return f"é”™è¯¯: {error_msg}"


@tool(
    "enhanced_web_crawler_tool",
    description="""ä»æŒ‡å®šURLæŠ“å–ç½‘é¡µå†…å®¹ã€‚
    å¯ä»¥æå–ç½‘é¡µçš„æ–‡æœ¬å†…å®¹ã€æ ‡é¢˜ç­‰ä¿¡æ¯ã€‚
    é€‚åˆç”¨äºè·å–ç‰¹å®šç½‘é¡µçš„è¯¦ç»†å†…å®¹ã€‚""",
)
def enhanced_web_crawler_tool(
    url: str = Field(..., description="è¦æŠ“å–çš„ç½‘é¡µURL"),
    timeout: int = Field(default=30, description="è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰"),
):
    """
    æŠ“å–ç½‘é¡µå†…å®¹

    Args:
        url: ç½‘é¡µURL
        timeout: è¶…æ—¶æ—¶é—´

    Returns:
        str: ç½‘é¡µå†…å®¹
    """
    logger.info(f"ğŸŒ å¼€å§‹æŠ“å–ç½‘é¡µ: {url}")

    try:
        import requests
        from bs4 import BeautifulSoup

        # å‘é€HTTPè¯·æ±‚
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }

        response = requests.get(url, headers=headers, timeout=timeout)
        response.raise_for_status()

        # è§£æHTML
        soup = BeautifulSoup(response.content, "html.parser")

        # æå–æ ‡é¢˜
        title = soup.find("title")
        title_text = title.get_text().strip() if title else "æ— æ ‡é¢˜"

        # ç§»é™¤è„šæœ¬å’Œæ ·å¼å…ƒç´ 
        for script in soup(["script", "style"]):
            script.decompose()

        # æå–æ–‡æœ¬å†…å®¹
        text = soup.get_text()

        # æ¸…ç†æ–‡æœ¬
        lines = (line.strip() for line in text.splitlines())
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        text = " ".join(chunk for chunk in chunks if chunk)

        # é™åˆ¶å†…å®¹é•¿åº¦
        if len(text) > 5000:
            text = text[:5000] + "...[å†…å®¹è¿‡é•¿å·²æˆªæ–­]"

        result = f"ç½‘é¡µæ ‡é¢˜: {title_text}\n\nURL: {url}\n\nå†…å®¹:\n{text}"
        logger.info(f"âœ… ç½‘é¡µæŠ“å–å®Œæˆ: {url}")
        return result

    except Exception as e:
        error_msg = f"æŠ“å–ç½‘é¡µå¤±è´¥: {str(e)}"
        logger.error(error_msg)
        return f"é”™è¯¯: {error_msg}"


@tool(
    "url_validator_tool",
    description="""éªŒè¯URLæ˜¯å¦æœ‰æ•ˆå¯è®¿é—®ã€‚
    æ£€æŸ¥URLçš„å¯è¾¾æ€§å’Œå“åº”çŠ¶æ€ã€‚""",
)
def url_validator_tool(
    url: str = Field(..., description="è¦éªŒè¯çš„URL"),
    timeout: int = Field(default=10, description="è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰"),
):
    """
    éªŒè¯URLæœ‰æ•ˆæ€§

    Args:
        url: è¦éªŒè¯çš„URL
        timeout: è¶…æ—¶æ—¶é—´

    Returns:
        str: éªŒè¯ç»“æœ
    """
    logger.info(f"ğŸ”— éªŒè¯URL: {url}")

    try:
        import requests

        response = requests.head(url, timeout=timeout, allow_redirects=True)
        status_code = response.status_code

        if 200 <= status_code < 300:
            result = f"URLæœ‰æ•ˆ: {url} (çŠ¶æ€ç : {status_code})"
            logger.info(f"âœ… URLéªŒè¯æˆåŠŸ: {url}")
        elif 300 <= status_code < 400:
            result = f"URLé‡å®šå‘: {url} (çŠ¶æ€ç : {status_code}) -> {response.url}"
            logger.info(f"â†©ï¸ URLé‡å®šå‘: {url}")
        else:
            result = f"URLè®¿é—®å¼‚å¸¸: {url} (çŠ¶æ€ç : {status_code})"
            logger.warning(f"âš ï¸ URLçŠ¶æ€å¼‚å¸¸: {url}")

        return result

    except Exception as e:
        error_msg = f"URLéªŒè¯å¤±è´¥: {str(e)}"
        logger.error(error_msg)
        return f"é”™è¯¯: {error_msg}"


@tool(
    "extract_links_tool",
    description="""ä»ç½‘é¡µä¸­æå–æ‰€æœ‰é“¾æ¥ã€‚
    å¯ä»¥æå–é¡µé¢ä¸­çš„æ‰€æœ‰è¶…é“¾æ¥ï¼Œä¾¿äºè¿›ä¸€æ­¥åˆ†æã€‚""",
)
def extract_links_tool(
    url: str = Field(..., description="è¦æå–é“¾æ¥çš„ç½‘é¡µURL"),
    link_type: str = Field(
        default="all",
        description="é“¾æ¥ç±»å‹ï¼šallï¼ˆæ‰€æœ‰ï¼‰, internalï¼ˆå†…éƒ¨ï¼‰, externalï¼ˆå¤–éƒ¨ï¼‰",
    ),
    timeout: int = Field(default=30, description="è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰"),
):
    """
    ä»ç½‘é¡µæå–é“¾æ¥

    Args:
        url: ç½‘é¡µURL
        link_type: é“¾æ¥ç±»å‹ç­›é€‰
        timeout: è¶…æ—¶æ—¶é—´

    Returns:
        str: æå–çš„é“¾æ¥åˆ—è¡¨
    """
    logger.info(f"ğŸ”— æå–é“¾æ¥: {url} (ç±»å‹: {link_type})")

    try:
        from urllib.parse import urljoin, urlparse

        import requests
        from bs4 import BeautifulSoup

        # å‘é€è¯·æ±‚
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }

        response = requests.get(url, headers=headers, timeout=timeout)
        response.raise_for_status()

        # è§£æHTML
        soup = BeautifulSoup(response.content, "html.parser")
        links = soup.find_all("a", href=True)

        base_domain = urlparse(url).netloc
        result_links = []

        for link in links:
            href = link["href"]  # type: ignore
            full_url = urljoin(url, href)  # type: ignore
            link_domain = urlparse(full_url).netloc
            link_text = link.get_text().strip()

            # æ ¹æ®ç±»å‹ç­›é€‰
            if link_type == "internal" and link_domain != base_domain:
                continue
            elif link_type == "external" and link_domain == base_domain:
                continue

            result_links.append(f"â€¢ {link_text}: {full_url}")

        if result_links:
            result = f"ä» {url} æå–çš„é“¾æ¥ ({len(result_links)} ä¸ª):\n\n" + "\n".join(
                result_links[:50]
            )
            if len(result_links) > 50:
                result += f"\n\n... è¿˜æœ‰ {len(result_links) - 50} ä¸ªé“¾æ¥æœªæ˜¾ç¤º"
            logger.info(f"âœ… é“¾æ¥æå–å®Œæˆ: {len(result_links)} ä¸ª")
        else:
            result = f"åœ¨ {url} ä¸­æœªæ‰¾åˆ°é“¾æ¥"

        return result

    except Exception as e:
        error_msg = f"æå–é“¾æ¥å¤±è´¥: {str(e)}"
        logger.error(error_msg)
        return f"é”™è¯¯: {error_msg}"


# ğŸ› ï¸
@tool(
    "enhanced_search_and_crawler_tool",
    description="æ­¤å·¥å…·ä½¿ç”¨æä¾›çš„å…³é”®å­—æ‰§è¡Œæœç´¢ï¼Œå¹¶è·å–ä»¥markdownæ ¼å¼æ˜¾ç¤ºçš„ä¿¡æ¯ã€‚",
)
def enhanced_search_and_crawler_tool(
    query: str = Field(description="è¦æœç´¢çš„å…³é”®å­—ã€‚"),
) -> List:
    try:
        logger.info(f">>>>>>> [search_tool] input -> query: {query}")
        response = enhanced_web_search_tool.run(query)

        if response and isinstance(response, list):
            for item in response:
                if "url" in item:
                    _url = item["url"]
                    crawled_content = enhanced_web_crawler_tool.run(_url)
                    item["content"] = crawled_content
            logger.info(f"<<<<<<< [search_tool] output -> : {response}")
            return response

        return response
    except BaseException as e:
        error_msg = f"Failed to Serp. Error: {repr(e)}"
        logger.error(error_msg)
        return [error_msg]

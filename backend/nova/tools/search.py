# -*- coding: utf-8 -*-
# @Time   : 2025/04/01 10:24
# @Author : zip
# @Moto   : Knowledge comes from decomposition
import asyncio
import json
import logging
import os
from typing import Dict, List

import requests
from crawl4ai import AsyncWebCrawler, BrowserConfig, CacheMode, CrawlerRunConfig
from langchain_core.tools import tool
from pydantic import Field

logger = logging.getLogger(__name__)


# ğŸ› ï¸
@tool("crawl_tool", description="ä½¿ç”¨æ­¤å·¥å…·çˆ¬ç½‘urlå¹¶ä»¥markdownæ ¼å¼è·å–å¯è¯»å†…å®¹ã€‚")
def crawl_tool(url: str = Field(description="è¦çˆ¬å–çš„url")) -> Dict:
    try:
        logger.info(f">>>>>>> [crawl_tool] input -> url: {url}")

        # Create a browser configuration with builtin mode
        browser_config = BrowserConfig(
            browser_mode="builtin",  # This is the key setting!
            headless=True,  # Can run headless for background operation
        )

        # Create crawler run configuration
        crawler_config = CrawlerRunConfig(
            cache_mode=CacheMode.BYPASS,  # Skip cache for this demo
            screenshot=True,  # Take a screenshot
            verbose=True,  # Show verbose logging
        )

        # å¼‚æ­¥çˆ¬å–ç½‘é¡µå†…å®¹
        async def fetch_content():
            async with AsyncWebCrawler(config=browser_config) as crawler:
                result = await crawler.arun(url=url, config=crawler_config)
                return result.markdown  # type: ignore

        # ä½¿ç”¨ asyncio.run è°ƒç”¨å¼‚æ­¥ä»»åŠ¡
        markdown_content = asyncio.run(fetch_content())

        message = {
            "title": f"å†…å®¹çˆ¬å–è‡ª: {url}",
            "content": markdown_content,
        }

        logger.info(f"<<<<<<< [crawl_tool] output -> : {message}")

        return message

    except BaseException as e:
        error_msg = f"Failed to crawl. Error: {repr(e)}"
        logger.error(error_msg)
        return {"title": f"å†…å®¹çˆ¬å–è‡ª: {url}", "content": error_msg}


# ğŸ› ï¸
@tool("serp_tool", description="ä½¿ç”¨æ­¤å·¥å…·åœ¨webä¸Šæœç´¢ä¿¡æ¯å¹¶è¿”å›åˆ—è¡¨")
def serp_tool(
    query: str = Field(..., description="æŸ¥è¯¢æœç´¢ï¼Œè·å–seoã€‚"),
) -> List:
    try:
        headers = {
            "X-API-KEY": os.getenv("GOOGLE_SERPER_API_KEY"),
            "Content-Type": "application/json",
        }
        logger.info(f">>>>>>> [serp_tool] input -> query: {query}")
        payload = json.dumps({"q": query, "num": 1})

        response = requests.post(
            os.getenv("GOOGLE_SERPER_URL"),  # type: ignore
            headers=headers,
            data=payload,  # type: ignore
            timeout=30.0,
        )
        response.raise_for_status()
        response = response.json()["organic"]
        # æ ¼å¼è½¬æ¢
        response = [
            {
                "title": item["title"],
                "url": item["link"],
                "content": item["snippet"],
                "score": item["position"],
            }
            for item in response
        ]

        # response = [
        #     {
        #         "title": "å¤©æ°”å®å†µä¸é¢„æŠ¥ - æ·±åœ³å¸‚æ°”è±¡å±€ï¼ˆå°ï¼‰",
        #         "url": "https://weather.sz.gov.cn/qixiangfuwu/yubaofuwu/index.html",
        #         "content": "æ·±åœ³å¸‚æ°”è±¡å±€é—¨æˆ·ç½‘ç«™ä¸ºæ‚¨æä¾›æƒå¨ã€åŠæ—¶ã€å‡†ç¡®çš„æ·±åœ³å¤©æ°”é¢„è­¦ã€å¤©æ°”é¢„æŠ¥ã€å¤©æ°”å®å†µã€å°é£è·¯å¾„ã€æ·±åœ³æ°”å€™ç­‰ä¿¡æ¯æœåŠ¡ï¼Œä¸ºæ·±åœ³åŠå…¶å‘¨è¾¹åŸå¸‚çš„ç”Ÿäº§ç”Ÿæ´»æä¾›å…¨é¢å¯é çš„æ°”è±¡ ...",
        #         "score": 1,
        #     }
        # ]
        logger.info(f"<<<<<<< [serp_tool] output -> : {response}")
        return response

    except BaseException as e:
        error_msg = f"Failed to Serp. Error: {repr(e)}"
        logger.error(error_msg)
        return [error_msg]


# ğŸ› ï¸
@tool(
    "search_tool",
    description="æ­¤å·¥å…·ä½¿ç”¨æä¾›çš„å…³é”®å­—æ‰§è¡Œæœç´¢ï¼Œå¹¶è·å–ä»¥markdownæ ¼å¼æ˜¾ç¤ºçš„ä¿¡æ¯ã€‚",
)
def search_tool(query: str = Field(description="è¦æœç´¢çš„å…³é”®å­—ã€‚")) -> List:
    try:
        logger.info(f">>>>>>> [search_tool] input -> query: {query}")
        response = serp_tool.run(query)

        if response and isinstance(response, list):
            for item in response:
                if "url" in item:
                    _url = item["url"]
                    crawled_content = crawl_tool.run(_url)
                    item["content"] = crawled_content
            logger.info(f"<<<<<<< [search_tool] output -> : {response}")
            return response

        return response
    except BaseException as e:
        error_msg = f"Failed to Serp. Error: {repr(e)}"
        logger.error(error_msg)
        return [error_msg]

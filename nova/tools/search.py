# -*- coding: utf-8 -*-
# @Time   : 2025/04/01 10:24
# @Author : zip
# @Moto   : Knowledge comes from decomposition
import json
import logging
import os
import re
from typing import List
from urllib.parse import urljoin

import requests
from langchain_core.tools import tool
from markdownify import markdownify as md
from pydantic import Field
from readabilipy import simple_json_from_html_string

logger = logging.getLogger(__name__)


class Article:
    def __init__(self, url, title, html_content):
        self.url = url
        self.title = title
        self.html_content = html_content

    def to_markdown(self, including_title: bool = True) -> str:
        markdown = ""
        if including_title:
            markdown += f"# {self.title}\n\n"
        markdown += md(self.html_content)
        return markdown

    def to_message(self) -> list[str | dict]:
        image_pattern = r"!\[.*?\]\((.*?)\)"

        content = []
        parts = re.split(image_pattern, self.to_markdown())

        for i, part in enumerate(parts):
            if i % 2 == 1:
                image_url = urljoin(self.url, part.strip())
                content.append({"type": "image_url", "image_url": {"url": image_url}})
            else:
                content.append({"type": "text", "text": part.strip()})

        return content


# ğŸ› ï¸
@tool("crawl_tool", description="ä½¿ç”¨æ­¤å·¥å…·çˆ¬ç½‘urlå¹¶ä»¥markdownæ ¼å¼è·å–å¯è¯»å†…å®¹ã€‚")
def crawl_tool(url: str = Field(description="è¦çˆ¬å–çš„url")) -> List:
    headers = {
        "Content-Type": "application/json",
        "X-Return-Format": "html",  # ä¸ºä½•ä¸ç”¨è‡ªå¸¦çš„format -> markdown, å‘ç°è‡ªå¸¦çš„æ•ˆæœä¸å¥½ï¼Œæ€»æ˜¯ä¸€å¤§ä¸²å­—ç¬¦ä¸²
    }
    if os.getenv("JINA_API_KEY"):
        headers["Authorization"] = f"Bearer {os.getenv('JINA_API_KEY')}"

    try:
        logger.info(f">>>>>>> [crawl_tool] input -> url: {url}")

        # response = requests.post(
        #     "https://r.jina.ai/", headers=headers, json={"url": url}
        # )
        # article = simple_json_from_html_string(response.text, use_readability=True)
        # article = Article(
        #     url=url,
        #     title=article.get("title"),
        #     html_content=article.get("content"),
        # )
        # message = article.to_message()

        message = [
            {
                "title": "å¤©æ°”å®å†µä¸é¢„æŠ¥ - æ·±åœ³å¸‚æ°”è±¡å±€ï¼ˆå°ï¼‰",
                "url": "https://weather.sz.gov.cn/qixiangfuwu/yubaofuwu/index.html",
                "content": [
                    {
                        "type": "text",
                        "text": "# å¤©æ°”å®å†µä¸é¢„æŠ¥-æ·±åœ³å¸‚æ°”è±¡å±€ï¼ˆå°ï¼‰\\n\\n**æ·±åœ³å›½å®¶åŸºæœ¬æ°”è±¡ç«™å®å†µ**\\n18:58",
                    },
                    {"type": "text", "text": "26.8\\n\\næ°”æ¸© â„ƒ"},
                    {"type": "text", "text": "ä¸œåŒ—ååŒ—é£ å°äºä¸‰çº§\\n\\né£"},
                ],
            }
        ]

        logger.info(f"<<<<<<< [crawl_tool] output -> : {message}")

        return message

    except BaseException as e:
        error_msg = f"Failed to crawl. Error: {repr(e)}"
        logger.error(error_msg)
        return [error_msg]


# ğŸ› ï¸
@tool("serp_tool", description="ä½¿ç”¨æ­¤å·¥å…·åœ¨webä¸Šæœç´¢ä¿¡æ¯å¹¶è¿”å›åˆ—è¡¨")
def serp_tool(
    query: str = Field(..., description="æŸ¥è¯¢æœç´¢ï¼Œè·å–seoã€‚"),
) -> List:
    try:
        # headers = {
        #     "X-API-KEY": os.getenv("GOOGLE_SERPER_API_KEY"),
        #     "Content-Type": "application/json",
        # }
        # logger.info(f">>>>>>> [serp_tool] input -> query: {query}")
        # payload = json.dumps({"q": query, "num": 1})

        # response = requests.post(
        #     os.getenv("GOOGLE_SERPER_URL"),  # type: ignore
        #     headers=headers,
        #     data=payload,  # type: ignore
        #     timeout=30.0,
        # )
        # response.raise_for_status()
        # response = response.json()["organic"]
        # # æ ¼å¼è½¬æ¢
        # response = [
        #     {
        #         "title": item["title"],
        #         "url": item["link"],
        #         "content": item["snippet"],
        #         "score": item["position"],
        #     }
        #     for item in response
        # ]

        response = [
            {
                "title": "å¤©æ°”å®å†µä¸é¢„æŠ¥ - æ·±åœ³å¸‚æ°”è±¡å±€ï¼ˆå°ï¼‰",
                "url": "https://weather.sz.gov.cn/qixiangfuwu/yubaofuwu/index.html",
                "content": "æ·±åœ³å¸‚æ°”è±¡å±€é—¨æˆ·ç½‘ç«™ä¸ºæ‚¨æä¾›æƒå¨ã€åŠæ—¶ã€å‡†ç¡®çš„æ·±åœ³å¤©æ°”é¢„è­¦ã€å¤©æ°”é¢„æŠ¥ã€å¤©æ°”å®å†µã€å°é£è·¯å¾„ã€æ·±åœ³æ°”å€™ç­‰ä¿¡æ¯æœåŠ¡ï¼Œä¸ºæ·±åœ³åŠå…¶å‘¨è¾¹åŸå¸‚çš„ç”Ÿäº§ç”Ÿæ´»æä¾›å…¨é¢å¯é çš„æ°”è±¡ ...",
                "score": 1,
            }
        ]
        logger.info(f"<<<<<<< [serp_tool] output -> : {response}")
        return response

    except BaseException as e:
        error_msg = f"Failed to Serp. Error: {repr(e)}"
        logger.error(error_msg)
        return [error_msg]


# ğŸ› ï¸
@tool(
    "search_tool",
    description="æ­¤å·¥å…·ä½¿ç”¨æä¾›çš„å…³é”®å­—æ‰§è¡Œæœç´¢ï¼Œå¹¶è·å–ä»¥markdownæ ¼å¼æ˜¾ç¤ºçš„ä¿¡æ¯ã€‚",
)
def search_tool(query: str = Field(description="è¦æœç´¢çš„å…³é”®å­—ã€‚")) -> List:
    try:
        logger.info(f">>>>>>> [search_tool] input -> query: {query}")
        response = serp_tool.run(query)

        if response and isinstance(response, list):
            for item in response:
                if "url" in item:
                    _url = item["url"]
                    crawled_content = crawl_tool.run(_url)
                    item["content"] = crawled_content
            logger.info(f"<<<<<<< [search_tool] output -> : {response}")
            return response

        return response
    except BaseException as e:
        error_msg = f"Failed to Serp. Error: {repr(e)}"
        logger.error(error_msg)
        return [error_msg]
